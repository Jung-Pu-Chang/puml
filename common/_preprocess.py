import datetime
import numpy as np
import pandas as pd
import sklearn
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import (
    LabelEncoder,
    OneHotEncoder,
    RobustScaler,
)
from scipy import stats
import warnings
import math

from ._base import BaseWithSeed

warnings.filterwarnings("ignore")


class PreProcess(BaseWithSeed):
    """
    è³‡æ–™å‰è™•ç†ä¸»é¡åˆ¥ï¼Œè² è²¬ä¾åºåŸ·è¡Œï¼š
    1. å‹åˆ¥è½‰æ›
    2. é¡åˆ¥ç·¨ç¢¼
    3. ç¼ºå¤±å€¼è£œå€¼ (KNN/MICE)
    4. ç‰¹å¾µç¸®æ”¾ (RobustScaler)
    5. ç•°å¸¸å€¼åµæ¸¬ (IsolationForest)
    6. è³‡æ–™æ¼‚ç§»
    7. çµ±è¨ˆæª¢é©—
    """

    def __init__(self, train, test, target_col, drop_col=None, seed: int = 17):
        super().__init__(seed)
        self.target_col = (
            [target_col] if isinstance(target_col, str) else list(target_col)
        )
        self.drop_col = (
            []
            if drop_col is None
            else ([drop_col] if isinstance(drop_col, str) else list(drop_col))
        )

        # train test åˆ‡åˆ†
        self.train = train.drop(columns=self.drop_col, errors="ignore")
        self.test = test.drop(columns=self.drop_col, errors="ignore")
        self.trainY = self.train[self.target_col]  # å–å‡º target æ¬„ä½
        self.trainX = self.train.drop(columns=self.target_col, errors="ignore")
        self.testX = self.test[self.trainX.columns]

        # train test åˆä½µ
        self.df_all = pd.concat([self.train, self.test], axis=0, ignore_index=True)

    # ---------- 1. è‡ªå‹•å‹åˆ¥è½‰æ› ----------
    def auto_type_convert(self):
        df_type = self.df_all.copy()
        for col in self.df_all.columns:
            if df_type[col].dtype == "object":
                try:
                    df_type[col] = pd.to_datetime(df_type[col])
                    print(f"{col}: è½‰ç‚º datetime")
                except Exception:
                    df_type[col] = df_type[col].astype("category")
                    print(f"{col}: è½‰ç‚º category")
            elif pd.api.types.is_numeric_dtype(df_type[col]):
                df_type[col] = df_type[col].astype(float)
        print("ğŸ” è‡ªå‹•è½‰æ›ç‰¹å¾µå‹æ…‹å®Œç•¢")
        return df_type

    # ---------- 2. ç·¨ç¢¼ï¼ˆOne-hot / Label / targetï¼‰ ----------
    def cat_encode(self, df_type, strategy="target", cat_cols=None):
        df_encoded = df.copy()

        if cat_cols is None:  # è‹¥æœªæŒ‡å®šï¼Œè‡ªå‹•æ‰¾ category dtype
            cat_cols = df_encoded.select_dtypes(
                include=["category", "object"]
            ).columns.tolist()

        if not cat_cols:
            print("âš ï¸ æœªåµæ¸¬åˆ°é¡åˆ¥æ¬„ä½ï¼Œç•¥éç·¨ç¢¼æ­¥é©Ÿã€‚")
            # return df_encoded

        for col in cat_cols:
            if strategy == "onehot":
                df_encoded = pd.get_dummies(df_encoded, columns=[col], prefix=col)
            elif strategy == "label":
                le = LabelEncoder()
                df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
            elif strategy == "target":  # æ³¨æ„ Data Leakage
                train_mapping_df = self.train  # åªæŠ“ train
                target_mean = (
                    train_mapping_df.groupby(col)[self.target_col].mean().reset_index()
                )
                target_mean.columns = [col, f"{col}_Target_Mean"]
                df_encoded = pd.merge(df_encoded, target_mean, on=col, how="inner")
                df_encoded = df_encoded.drop(col, axis=1)
            else:
                raise ValueError(f"æœªçŸ¥çš„ç·¨ç¢¼ç­–ç•¥ï¼š{strategy}")
        print(f"ğŸ¨ ä½¿ç”¨ {strategy} è½‰æ›é¡åˆ¥ç‰¹å¾µå®Œç•¢")
        return df_encoded

    # ---------- 3. ç¼ºå¤±å€¼è£œå€¼ï¼ˆç¼ºå¤±ç‡ na_rate < 0.1 è£œå€¼ï¼Œå¦å‰‡åˆªé™¤è©²æ¬„ä½ï¼‰ ----------
    def sparse_fill_na(
        self, df_encoded, na_rate=0.1, algorithm="knn", n_neighbors=None
    ):
        chk = df_encoded.isnull().mean().reset_index()
        chk.columns = ["æ¬„ä½åç¨±", "ç¼ºå¤±ç‡"]
        fillna_col = chk.loc[chk["ç¼ºå¤±ç‡"] <= na_rate, "æ¬„ä½åç¨±"].tolist()

        if not fillna_col:
            raise ValueError("æ²’æœ‰æ¬„ä½ç¬¦åˆ na_rate æ¢ä»¶å¯é€²è¡Œè£œå€¼ã€‚")

        fillna_df = df_encoded[fillna_col].copy()

        if algorithm == "mice":
            if n_neighbors is not None:
                print("âš ï¸ n_neighbors åªåœ¨ KNN æ¨¡å¼æœ‰æ•ˆï¼ŒMICE æ¨¡å¼æœƒå¿½ç•¥æ­¤åƒæ•¸")
            imputer = IterativeImputer(random_state=self.seed)
        elif algorithm == "knn":
            imputer = KNNImputer(n_neighbors=n_neighbors)
        else:
            raise ValueError("algorithm å¿…é ˆæ˜¯ 'knn' æˆ– 'mice'")

        # åˆªé™¤é æ¸¬ç›®æ¨™ï¼Œå†è£œå€¼
        cols = [c for c in fillna_df.columns if c not in self.target_col]
        fillna_df = pd.DataFrame(imputer.fit_transform(fillna_df[cols]), columns=cols)

        fillna_df["type"] = np.repeat(
            ["train", "test"], [len(self.trainX), len(self.testX)]
        )
        print(f"ğŸ§© ä½¿ç”¨ {algorithm.upper()} è£œå€¼å®Œç•¢")
        return pd.concat([fillna_df, df_encoded[self.target_col]], axis=1)

    # ---------- 4. ç‰¹å¾µè½‰æ› robust æ¶ˆé›¢ç¾¤å€¼ ----------
    def robust_scaling(self, fillna_df):
        fillna_df = fillna_df.drop(self.target_col, axis=1)
        df_train = fillna_df.loc[fillna_df["type"] == "train"].reset_index(drop=True)
        df_test = fillna_df.loc[fillna_df["type"] == "test"].reset_index(drop=True)
        num_cols = df_train.select_dtypes(include="number").columns
        scaler = RobustScaler()
        scaled_train = pd.DataFrame(
            scaler.fit_transform(df_train[num_cols]), columns=num_cols
        )

        scaled_test = pd.DataFrame(
            scaler.transform(df_test[num_cols]), columns=num_cols
        )

        df_train_scaled = pd.concat(
            [scaled_train, df_train.drop(columns=num_cols)], axis=1
        )
        df_test_scaled = pd.concat(
            [scaled_test, df_test.drop(columns=num_cols)], axis=1
        )
        scaled_df = pd.concat([df_train_scaled, df_test_scaled])

        print("âœ… Robust scaling å®Œæˆ")
        return pd.concat([scaled_df, self.trainY], axis=1)

    # ---------- 5. anomaly detect (IsolationForest)ï¼Œä¸¦å›å‚³å»é™¤ anomaly å¾Œ train data ----------
    def anomaly_detect(self, scaled_df, max_features=1.0, contamination="auto"):
        # æ³¨æ„ï¼šé æ¸¬ç›®æ¨™ä¸èƒ½æ”¾å…¥
        df_train = (
            scaled_df.loc[scaled_df["type"] == "train"]
            .drop(target_col + ["type"], axis=1)
            .reset_index(drop=True)
        )
        n_estimators = max(
            100, min(2000, 200 * max(1, int(round(math.log2(max(2, len(df_train)))))))
        )  # å»ºè­°æ¨¹é‡
        model = IsolationForest(
            n_estimators=n_estimators,
            max_samples="auto",
            contamination=contamination,  # ä¸Ÿæ‰å¤šå°‘æ¯”ä¾‹çš„ç•°å¸¸å€¼
            max_features=max_features,  # æ¯æ£µæ¨¹çœ‹å¤šå°‘ç‰¹å¾µï¼Œ1.0 = 100%
            random_state=self.seed,
        )
        model.fit(df_train)
        preds = model.predict(df_train)
        df_train["anomaly"] = preds
        n_removed = int((preds == -1).sum())
        percentage = n_removed / len(df_train) * 100
        df_train[self.target_col] = scaled_df.loc[
            scaled_df["type"] == "train", self.target_col
        ].reset_index(drop=True)
        df_train = df_train.loc[df_train["anomaly"] == 1].reset_index(drop=True)

        print(f"ç§»é™¤è¨“ç·´è³‡æ–™ {n_removed} ç•°å¸¸å€¼ï¼Œç´„ {percentage:.1f}%")

        df_test = fillna_df.loc[fillna_df["type"] == "test"].reset_index(drop=True)
        df_clean = pd.concat([df_train, df_test], ignore_index=True)
        df_clean["type"] = ["train"] * len(df_train) + ["test"] * len(df_test)
        return df_clean.drop(["anomaly"], axis=1)

    # ---------- 6. train vs test similarity (KS test) ----------
    def data_drift(self, df_clean):
        results = []
        train_df = df_clean.loc[df_clean["type"] == "train"].drop(["type"], axis=1)
        test_df = df_clean.loc[df_clean["type"] == "test"].drop(["type"], axis=1)

        for col in train_df.select_dtypes(include=["number"]).columns:
            try:
                stat, p_val = stats.ks_2samp(
                    train_df[col].fillna(0), test_df[col].fillna(0)
                )
            except Exception:
                stat, p_val = np.nan, np.nan
            results.append(
                [col, stat, p_val, 0 if (not np.isnan(p_val) and p_val < 0.05) else 1]
            )

        print("ğŸ” check data_drift with KS-test")
        return pd.DataFrame(
            results, columns=["ç‰¹å¾µ", "KSçµ±è¨ˆé‡", "p-value", "æ˜¯å¦ç›¸ä¼¼"]
        )

    # ---------- 7. hypothesis tests (AD + Pearson) ----------
    def stat_summary_ad_pearson(self, df_clean):
        chk = pd.DataFrame(
            columns=["æ¬„ä½åç¨±", "ad_stat", "æ˜¯å¦å¸¸æ…‹", "ç¨ç«‹æ•¸", "ä¸ç¨ç«‹æ•¸", "ç¨ç«‹ç‡"]
        )
        df_train = df_clean.loc[df_clean["type"] == "train"]
        df_train = df_train.drop(columns=["type"] + self.target_col, errors="ignore")

        for col in df_train.columns:
            try:
                result = stats.anderson(df_train[col].dropna(), dist="norm")
                stat_value = result.statistic
                critical_value = (
                    result.critical_values[2]
                    if len(result.critical_values) > 2
                    else result.critical_values[-1]
                )
                is_normal = 1 if stat_value < critical_value else 0
            except Exception:
                stat_value = np.nan
                is_normal = 0

            independent_count = 0
            dependent_count = 0
            for other_col in df_train.columns:
                if col == other_col:
                    continue
                try:
                    corr, p_val = stats.pearsonr(
                        df_train[col].fillna(0), df_train[other_col].fillna(0)
                    )
                    if p_val < 0.05:
                        dependent_count += 1
                    else:
                        independent_count += 1
                except Exception:
                    pass
            total_comparisons = max(1, len(df_train.columns) - 1)
            independent_rate = independent_count / total_comparisons
            chk = pd.concat(
                [
                    chk,
                    pd.DataFrame(
                        {
                            "æ¬„ä½åç¨±": [col],
                            "ad_stat": [stat_value],
                            "æ˜¯å¦å¸¸æ…‹": [is_normal],
                            "ç¨ç«‹æ•¸": [independent_count],
                            "ä¸ç¨ç«‹æ•¸": [dependent_count],
                            "ç¨ç«‹ç‡": [round(independent_rate, 2)],
                        }
                    ),
                ],
                ignore_index=True,
            )
        print("ğŸ“Š check train_x and train_y by adã€pearson test ")
        return chk
